{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing word2vec embeddings by demostrating clustering.\n",
    "\n",
    "The entire assignment can be split up into subsections:\n",
    "- Install dependencies\n",
    "- Preprocessing the subtitle file to generate tokens\n",
    "- Training the model over these tokens\n",
    "- Running the clustering algorithm and assessing the clusters formed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scipy==1.10.1\n",
    "%pip install scikit-learn gensim nltk pysrt bs4 contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For preprocessing we need the following libraries\n",
    "import string\n",
    "import pysrt\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# For training the model we need the following library\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# For clustering we need the following library\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Text\n",
    "We first need to preprocess text, since it will taken from a subtitle file. The goal here is to convert text from a subtitle `.srt` file to an array of sentences, where each sentence is an array of words. This will be the input to our model.\n",
    "\n",
    "We've employed various methods and used multiple packages to achieve this preprocessing, including the following:\n",
    "- We've used `pysrt` package to extract the text from the subtitle file\n",
    "- but then we also need to clear out the html tags (`<i>...<\\i>`),\n",
    "- contractions (`i'll` should be converted to `i will`),\n",
    "- hypenated words (`25-to-1` should be reduced to `25`, `to`, `1`),\n",
    "- remove stopwords (words such as i, he, she, am, etc. that aren't useful to our analysis).\n",
    "- We tokenize the text with `nltk` library, and create an array of sentences, where each sentence is an array of words.\n",
    "- We also _lemmentize_ the words, so plurals of words are reduced to the singular form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hiya', 'frank'], ['wife', 'kid'], ['know', 'considering', 'treasury', 'bond', 'utility', 'stock'], ['late', \"'70s\", 'banking', 'job', 'went', 'make', 'large', 'sum', 'money'], ['fucking', 'snooze']]\n",
      "1654\n"
     ]
    }
   ],
   "source": [
    "class PreprocssText:\n",
    "    def __init__(self):\n",
    "        self.text = ''\n",
    "        self.stop_words = set(stopwords.words('english') + list(string.punctuation) + ['...', '``', '\\'\\'', '\\'s', 'us'])\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "\n",
    "    def separate_hypenated_word(self, sentence):\n",
    "        ret_sentence = []\n",
    "        for word in sentence:\n",
    "            if '-' in word:\n",
    "                parts = word.split('-')\n",
    "                for part in parts:\n",
    "                    if part:\n",
    "                        ret_sentence.append(part)\n",
    "            else:\n",
    "                ret_sentence.append(word)\n",
    "        return ret_sentence\n",
    "\n",
    "    def lemmatize(self, sentence):\n",
    "        ret_sentence = []\n",
    "        for word in sentence:\n",
    "            lem_word = self.wnl.lemmatize(word)\n",
    "            if lem_word == 'cdos':\n",
    "                lem_word = 'cdo'\n",
    "            ret_sentence.append(lem_word)\n",
    "        return ret_sentence\n",
    "\n",
    "    def tokenize(self, remove_stopwords=True, lemmatize=True):\n",
    "        # Tokenize into sentences\n",
    "        sentences = sent_tokenize(self.text)\n",
    "\n",
    "        # Tokenize each sentence into words and remove stopwords\n",
    "        tokenized_sentences = []\n",
    "        for sentence in sentences:\n",
    "            words = word_tokenize(sentence)\n",
    "            words = [word.lower() for word in words if word and not word.isdigit()]\n",
    "            words = self.separate_hypenated_word(words)\n",
    "            if remove_stopwords:\n",
    "                words = [word for word in words if word not in self.stop_words]\n",
    "            if lemmatize:\n",
    "                words = self.lemmatize(words)\n",
    "            if len(words)>1:\n",
    "                tokenized_sentences.append(words)\n",
    "        return tokenized_sentences\n",
    "\n",
    "    def read_file(self, file_path):\n",
    "        subs = pysrt.open(file_path, encoding='utf-8')\n",
    "\n",
    "        # Extract text from subtitle objects and remove HTML tags\n",
    "        text = ' '.join([sub.text for sub in subs])\n",
    "        soup = BeautifulSoup(text, 'html.parser')\n",
    "        for font_tag in soup.find_all('font'):\n",
    "            font_tag.decompose()\n",
    "        clean_text = soup.get_text()\n",
    "\n",
    "        clean_text = contractions.fix(clean_text)\n",
    "        self.text = clean_text\n",
    "        return self\n",
    "\n",
    "# Path to the subtitle file\n",
    "subtitle_path = \"big_short.srt\"\n",
    "tokens = PreprocssText().read_file(subtitle_path).tokenize()\n",
    "print(tokens[:5], len(tokens), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is an embedding?\n",
    "To be able to run computation over language, it is often quite convineint to create an embedding - a mapping - from words to a mathematical entity, here an n-dimensional vector. \n",
    "\n",
    "<img src=\"images/word_to_vector.png\" width=\"500\">\n",
    "\n",
    "We want to encode meaning with this vector representation. The words that are used in similar context are closer in the vector space, so for example `king` is closer to `man` than to `cat`. But their direction and magnitude also encodes information, that allows us to add to vector for `man` with the vector `queen - king` to get `woman`.\n",
    "\n",
    "<img src=\"images/king_example.png\" width=\"500\">\n",
    "\n",
    "# Generating word embeddings\n",
    "To generate word embeddings, we use the `gensim` library's `Word2Vec` class.\n",
    "\n",
    "What it does is that it trains a simple neural network to predict words that follows certain words, and then words that have similar weights in the model are the words that are used in similar context (the words following or preceding them are similar). This gets us the required embeddings.\n",
    "\n",
    "<img src=\"images/gnn.png\" width=\"500\">\n",
    "\n",
    "<img src=\"images/ww.png\" width=\"500\">\n",
    "\n",
    "Among the two methods present in this class, we use the first one, continuous-bag-of-words approach. \n",
    "\n",
    "<img src=\"images/cbow.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bond', 0.6647899746894836), ('wall', 0.6568737626075745), ('mortgage', 0.6547439098358154), ('housing', 0.6545563340187073), ('morgan', 0.6457417011260986), ('know', 0.6447020173072815), ('get', 0.6384490132331848), ('year', 0.6329621076583862), ('bank', 0.6312693357467651), ('right', 0.6299655437469482)]\n",
      "[('bond', 0.691154420375824), ('going', 0.6875411868095398), ('wall', 0.6870430111885071), ('housing', 0.6826854348182678), ('subprime', 0.6760833859443665), ('people', 0.6741327047348022), ('short', 0.6736552119255066), ('know', 0.6720733642578125), ('morgan', 0.6694979071617126), ('get', 0.6644611954689026)]\n"
     ]
    }
   ],
   "source": [
    "# Train the model \n",
    "#   vector_size: size of the word vectors. The more the better, \n",
    "#       but also the more computationally expensive.\n",
    "#   window: maximum distance between the current and predicted word within a sentence. \n",
    "#       Set to 100, so entire sentences are considered.\n",
    "#   min_count: ignore all words with total frequency lower than this.\n",
    "#   workers: number of worker threads to train the model. \n",
    "#       1 Here, to keep the output replicable.\n",
    "model = Word2Vec(tokens, vector_size=2000, window=100,  min_count=5, workers=1)\n",
    "\n",
    "# Extract word embeddings\n",
    "word_vectors = model.wv\n",
    "\n",
    "# Get word and corresponding vector\n",
    "word_vector_list = [word_vectors[word] for word in word_vectors.key_to_index.keys()]\n",
    "words_list = list(word_vectors.key_to_index.keys())\n",
    "\n",
    "# Qualitatively assess the word embedding generated\n",
    "print(word_vectors.most_similar_cosmul('cdo')) \n",
    "print(word_vectors.most_similar_cosmul('mortgage')) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of the results\n",
    "Since the movie `Big Short` is about the 2008 housing crisis, we expect a lot of talk regarding CDOs and mortgages. The output is illustrative of this. CDOs were a type of bond, formed by morgages, and were shorted by the main characters by purchase of swaps. `wall` in the output is mostly from Wall Street, name of the exchange."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Clustering\n",
    "We use K-means algorithm to cluster the embeddings. We create 20 clusters, and then run some qualitative tests to see how words are clustered together. Words that are functionally similar, like a set of verbs, or a set of nouns that could be used interchangeable, should be clustered together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/droid/.local/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: ['bet', 'rate', 'face', 'wanted']\n",
      "Cluster 1: ['price', 'find', 'wait', 'heard', 'hell', 'banker', 'another', 'rickert']\n",
      "Cluster 2: ['want', 'make', 'time', 'big', 'thing', 'jamie', 'loss']\n",
      "Cluster 3: ['would', 'got', 'way', 'maybe', 'vinnie', 'business', 'position', 'mike', 'saying', 'kathy', 'help', 'backed', 'real', 'sold', 'merrill', 'taking', 'looking', 'stop', 'many', 'started', 'four', 'fact', 'return', 'leave', 'thousand', 'old', 'lynch', 'boy', 'deutsche', 'filled', 'dollar', 'worth', 'goldman', 'close']\n",
      "Cluster 4: ['talk', 'aaa', 'new', 'hear', 'getting', 'friend', 'insurance', 'minute', 'caught']\n",
      "Cluster 5: ['see', 'take', 'always', 'yes', 'back', 'made', 'bad', 'bubble', 'lawrence', 'thank', 'question', 'interest', 'hate', 'nice', 'name', 'bbb', 'part', 'stearns', 'collapse', 'sorry', 'anyone', 'everybody', 'nobody', 'lewis', 'paying', 'kind', 'greenspan', 'capital', 'frontpoint', 'american', 'lost', 'everyone', 'gentleman', 'today', 'together', 'sir', 'jesus', 'nothing', 'yet', 'honey', 'option', 'profit', 'buying', 'almost', 'seem', 'book', 'fish', 'small']\n",
      "Cluster 6: ['bear', 'value', 'please', 'charlie', 'economy', 'investment', 'fraud']\n",
      "Cluster 7: ['fund', 'default', 'buy', 'later', 'god', 'late', 'hand', 'happened', 'might', 'trying', 'payment', 'rich', 'asking', 'must']\n",
      "Cluster 8: ['yeah', 'market', 'let', 'sell', 'billion', 'come', 'world', 'said', 'home', 'financial', 'stock', 'talking', 'trade', 'agency']\n",
      "Cluster 9: ['bond', 'mortgage', 'get', 'one', 'swap', 'short', 'subprime', 'day', 'loan', 'morgan', 'rating']\n",
      "Cluster 10: ['go', 'fucking', 'okay', 'look', 'never', 'even', 'tell', 'much', 'credit', 'job', 'every', 'lose', 'little', 'feel', 'else', 'ever', 'sound', 'already', 'adjustable', 'department', 'left', 'dumb', 'bonus', 'check']\n",
      "Cluster 11: ['like', 'good', 'need', 'baum', 'man', 'jared', 'fail', 'took', 'betting', 'three', 'write', 'working']\n",
      "Cluster 12: ['call', 'mean', 'give', 'fuck', 'number', 'dr.', 'work', 'first', 'aa', 'still', 'stanley', 'tranche', 'system', 'investor', 'dog', 'brother', 'eye', 'trust', 'wife', 'fine', 'told', 'office', 'hi', 'dude', 'explain', 'long', 'anybody', 'month', 'level', 'risky', 'also']\n",
      "Cluster 13: ['security', 'second', 'everything', 'mr.', 'start', 'crazy', 'understand', 'lot', 'banking', 'done', 'anymore', 'end', 'underlying', 'found', 'gone', 'meet', 'somebody', 'chance']\n",
      "Cluster 14: ['guy', 'sure', 'last', 'risk', 'simple', 'try', 'sec', 'supposed']\n",
      "Cluster 15: ['know', 'going', 'right', 'bank', 'people', 'cdo', 'think', 'say', 'mark', 'money', 'year', 'shit', 'housing', 'wrong', 'pay', 'burry', 'two', 'whole', 'street', 'wall', 'b']\n",
      "Cluster 16: ['million', 'ben', 'hey', 'well', 'michael', 'something', 'thought', 'called', 'zero', 'selling', 'ago', 'read', 'chris']\n",
      "Cluster 17: ['really', 'actually', 'vennett', 'love', 'synthetic', 'house', '1', 'idea', 'company', 'kid', 'sense', 'poor', 'crisis', 'america', 'math', 'rated', 'solid']\n",
      "Cluster 18: ['could', 'deal', 'oh', 'went', 'put', 'pretty', 'someone', 'since', 'word', 'hold', 'contract']\n",
      "Cluster 19: ['great', 'holy', 'coming', 'care', 'keep', 'meeting', 'problem']\n"
     ]
    }
   ],
   "source": [
    "# Apply K-means clustering\n",
    "kmeans = KMeans(n_clusters=20, random_state=13)  # Fixing random state so the result are replicable\n",
    "kmeans.fit(word_vector_list)\n",
    "clusters = kmeans.labels_.tolist()\n",
    "\n",
    "# Associate words with their clusters\n",
    "clustered_words = {cluster: [] for cluster in set(clusters)}\n",
    "for i, word in enumerate(words_list):\n",
    "    clustered_words[clusters[i]].append(word)\n",
    "\n",
    "# Print words in each cluster\n",
    "for cluster, words in clustered_words.items():\n",
    "    print(f\"Cluster {cluster}: {words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To qualitatively analyze the clusters formed, we check the cluster the following words are assigned to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default is in Cluster 7\n",
      "stock is in Cluster 8\n",
      "market is in Cluster 8\n",
      "bond is in Cluster 9\n",
      "subprime is in Cluster 9\n",
      "mortgage is in Cluster 9\n",
      "short is in Cluster 9\n",
      "swap is in Cluster 9\n",
      "loan is in Cluster 9\n",
      "cdo is in Cluster 15\n",
      "money is in Cluster 15\n",
      "wall is in Cluster 15\n",
      "street is in Cluster 15\n",
      "bank is in Cluster 15\n"
     ]
    }
   ],
   "source": [
    "words_to_check = [\"cdo\", \"bond\", \"subprime\", \"mortgage\", \"default\", \"short\", \"swap\", \"money\", \"loan\", \"stock\", \"market\", \"wall\", \"street\", \"bank\"]\n",
    "for cluster, words in clustered_words.items():\n",
    "    for word in words_to_check:\n",
    "        if word in words:\n",
    "            print(f\"{word} is in Cluster {cluster}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of the results\n",
    "The results are illustrative of the financial instruments named throught the movie. Various words that are used together appear the same cluster, for example wall street, stock market, subprime mortgages, mortgage bonds, bank and money, short and swaps. \n",
    "\n",
    "Throughout the movie the protagonists were buying up credit default swaps, but we see in the above output default and swap are in separate clusters. This is probably because default is also used in another context, i.e., when someone becomes delinquent.\n",
    "\n",
    "The clustering clearly isn't perfect though, given we would expect stock market and wall street to be clustered together, and also many other words clustered with them aren't as similar."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
